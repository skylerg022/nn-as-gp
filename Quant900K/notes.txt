
Gridsearch Notes
* Note: The best performing test RMSE's for this non-Gaussian dataset are about 0.0208 according to Figure S4 in
  the supplementary material for the paper "Competition on Spatial Statistics for Large Datasets" by Huang et al.


General


X,Y NN
## Lee2018
- No more than 4 layers. The deeper, the shallower it should be.
- Batch size of 64/128 better
- Best val RMSE 0.18
  - 8 layers 256 wide, early stop at 28 epochs, batch size 32, small learning rate and weight decay

## Custom
- 4/8 hidden layers 256/512 wide had best performance
- Batch sizes of 64/128 yielded smaller RMSE w/o increasing chances of getting extremely high RMSE
- Deep, narrow (width <= 128) nets frequently converged to two different val RMSE's
- Best val RMSE 0.15
  - 8 layers 256 wide, early stop at 31 epochs, batch size 256

X,Y Transformed NN
## Lee2018
- 1/2 layers, >= 512 width best performance
- Batch size 128/256 best
- Best val RMSE 0.18
  - 4 layers 512 wide, only trained 30 epochs, batch size 256

## Custom
- (layers & width SAME AS X,Y NN CUSTOM)
- Batch size of 64/256 best
- Best val RMSE 0.13
  - 8 layers 256 wide, early stop at 35 epochs, batch size 256, has decay rate (0.1 / floor(n_train/batch_size))

Basis 4by4 NN
## Lee2018
- 1 layer, 1024 best
- Batch size of 256 best
- Bes val RMSE 0.14
  - 8 layers 256 wide, best epoch at 29, batch size 256

## Custom
- 2/4 layers >= 256 width best performance
- Batch size of 256 best
- Best val RMSE 0.11
  - 4 layers 512 wide, early stop at 42 epochs, batch size 128, has decay rate

Basis 4by4,20by20 NN
## Lee2018
- 1/2 layers of any width best
- Batch size of 128/256 best
- Best val RMSE 0.09
  - 4 layers 512 wide, best epoch at 28, batch size 128

## Custom
- 1/2/4 layers, >= 256 width best performance
- Batch size of 256 best
- Best val RMSE 0.08
  - 8 layers 256 wide, early stop at 41 epochs, batch size 256, has decay rate

