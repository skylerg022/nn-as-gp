
Gridsearch Notes

General
- All Lee2018 gridsearches had a handful of NN setups that failed NN setup.
  The learning rates of these NNs were much higher (e^(-4), 1/e) than those
  NNs that fit (e^(-10), e^(-1)).
- RMSE suffers more often for NNs with learning rate larger than e^(-3) or
  weight decay larger than e^(-5). Performance is MUCH worse for both high
  learning rate and weight decay.
- Deep NNs (8 and 16 hidden layers) had convergence to two val RMSE's

Lat-Long NN
## Lee2018
- 1 or 2 layers, any width between 64 and 2048 great performance
- Lower batch sizes tend to have lower RMSE
- Best val RMSE 1.19
  - 16 layers 128 wide, low learning rate, batch size 16
## Custom
- Layer width of 64/128 better than 8, depth does not matter
- Batch size of 64 generally has better RMSE
- Best val RMSE 1.23
  - 16 layers 128 wide, 100 epochs, batch size 256

Lat-Long Transformed NN
## Lee2018
- (SAME AS LAT-LONG NN)
- Best val RMSE 1.24
  - 2 layers 128 wide, batch size 128
## Custom
- (SAME AS LAT-LONG NN)
- Best val RMSE 1.24
  - 16 layers 64 wide, 100 epochs, batch size 64

Basis 4by4 NN
- Lower batch sizes tend to have lower RMSE
## Lee2018
- 1 or 3 layers, any width between 128 and 2048 great performance
- Best val RMSE 1.22
  - 10 layers 256 wide, batch size 16
## Custom
- Layer widths bigger than 8 (128-1024) give better performance
- Best RMSE 1.26
  - 8 layers 128 wide, batch size 64, dropout rate 0.1

Basis 20by20 NN
## Lee2018
- 1 or 3 layers, any width between 128 and 2048 great performance
- Best val RMSE 1.22
  - 3 layers 512 wide, batch size 64
## Custom
- 1 layer limited in performance but not the worst
- Lower batch size better
- Best val RMSE 1.41
  - 8 layers 128 wide, batch size 64, dropout 0.1

