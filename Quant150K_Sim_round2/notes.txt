Gridsearch Notes
* Note: The best test RMSE for this dataset is ___ and comes from 


General


X,Y NN
## Lee2018
- 1/2 layers, any width
- Effects of weight decay and learning rate problems most evident when both are high
- Best val RMSE 1.28
  - model_num 592, 4 layers 64 wide, batch size 32, early stop at 14 epochs

## Custom
- At least 4 layers and at least 64 wide for better avg. performance
- Batch size of 32 achieved lowest RMSEs
- Best val RMSE 1.27
  - model_num 43, 8 layers 256 wide, batch size 32, no lr decay or dropout, early stop at 12 epochs

X,Y Transformed NN
## Lee2018
- (SAME AS NN Lee2018)
- Best val RMSE 1.27
  - model_num 472, 8 layers 64 wide, batch size 32, early stop at 11 epochs

## Custom
- (SAME AS NN Custom)
- Batch size of 16 had lowest RMSEs
- Best val RMSE 1.26
  - model_num 15, 16 layers 128 wide, batch size 16, early stop at 9 epochs


Basis 4by4 NN
## Lee2018
- (SAME AS NN Lee2018)
- Best val RMSE 1.20
  - model_num 378, 16 layers 64 wide, batch size 16, early stop at 20 epochs

## Custom
- Any depth at width of 64 or more
- Batch size not 256
- Best val RMSE 1.24
  - model_num 310, 4 layers 512 wide, batch size 64, no decay, dropout 0.1, early stop at 17 epochs

Basis 4by4,20by20 NN
## Lee2018
- (SAME AS NN Custom)
- Best val RMSE 1.25
  - model_num 428, 4 layers 128 wide, batch size 128, early stop at 4 epochs

## Custom
- 2 or more layers 64 or more wide best on avg
- Best val RMSE 1.30
  - model_num 138, 4 layers 256 wide, batch size 16, has decay, no dropout, early stop at 12 epochs
